{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "488250d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running  >>  run\n",
      "painting  >>  paint\n",
      "walking  >>  walk\n",
      "dressing  >>  dress\n",
      "likely  >>  like\n",
      "children  >>  children\n",
      "whom  >>  whom\n",
      "good  >>  good\n",
      "ate  >>  ate\n",
      "fishing  >>  fish\n"
     ]
    }
   ],
   "source": [
    "# Exersize 1 : Convert these list of words into base form using Stemming and Lemmatization and observe the transformations\n",
    "\n",
    "#Stemming using nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "lst_words = ['running', 'painting', 'walking', 'dressing', 'likely', 'children', 'whom', 'good', 'ate', 'fishing']\n",
    "\n",
    "for word in lst_words:\n",
    "    print(word , \" >> \", ps.stem(word))\n",
    "\n",
    "#stemming using nltk gives the base form of the word by removing the suffixes and prefixes,\n",
    "# but it does not always give a valid word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd7407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running  >>  run  | \n",
      "painting  >>  painting  | \n",
      "walking  >>  walking  | \n",
      "dressing  >>  dress  | \n",
      "likely  >>  likely  | \n",
      "children  >>  child  | \n",
      "who  >>  who  | \n",
      "good  >>  good  | \n",
      "ate  >>  eat  | \n",
      "fishing  >>  fish  | \n"
     ]
    }
   ],
   "source": [
    "#lemmatization using spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"running painting walking dressing likely children who good ate fishing\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \" >> \", token.lemma_, \" | \")\n",
    "    \n",
    "    #lemmetization using spacy gives the base form of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23581f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 : convert the given text into it's base form using both stemming and lemmatization\n",
    "\n",
    "text = \"\"\"Latha is very multi talented girl.She is good at many skills like dancing, running, singing, playing.She also likes eating Pav Bhagi. she has a \n",
    "habit of fishing and swimming too.Besides all this, she is a wonderful at cooking too.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab7182dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\UPENDRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\UPENDRA/nltk_data'\n    - 'c:\\\\Users\\\\UPENDRA\\\\OneDrive\\\\Desktop\\\\nlp_python\\\\nlp_learning\\\\nlp_venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\UPENDRA\\\\OneDrive\\\\Desktop\\\\nlp_python\\\\nlp_learning\\\\nlp_venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\UPENDRA\\\\OneDrive\\\\Desktop\\\\nlp_python\\\\nlp_learning\\\\nlp_venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\UPENDRA\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#stemming using nltk \u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#word tokenization\u001b[39;00m\n\u001b[32m      5\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\UPENDRA\\OneDrive\\Desktop\\nlp_python\\nlp_learning\\nlp_venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\UPENDRA\\OneDrive\\Desktop\\nlp_python\\nlp_learning\\nlp_venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\UPENDRA\\OneDrive\\Desktop\\nlp_python\\nlp_learning\\nlp_venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\UPENDRA\\OneDrive\\Desktop\\nlp_python\\nlp_learning\\nlp_venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\UPENDRA\\OneDrive\\Desktop\\nlp_python\\nlp_learning\\nlp_venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\UPENDRA\\OneDrive\\Desktop\\nlp_python\\nlp_learning\\nlp_venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\UPENDRA/nltk_data'\n    - 'c:\\\\Users\\\\UPENDRA\\\\OneDrive\\\\Desktop\\\\nlp_python\\\\nlp_learning\\\\nlp_venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\UPENDRA\\\\OneDrive\\\\Desktop\\\\nlp_python\\\\nlp_learning\\\\nlp_venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\UPENDRA\\\\OneDrive\\\\Desktop\\\\nlp_python\\\\nlp_learning\\\\nlp_venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\UPENDRA\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#stemming using nltk \n",
    "\n",
    "#word tokenization\n",
    "\n",
    "nltk.download('punkt')\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8b02575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latha  >>  Latha  | \n",
      "is  >>  be  | \n",
      "very  >>  very  | \n",
      "multi  >>  multi  | \n",
      "talented  >>  talented  | \n",
      "girl  >>  girl  | \n",
      ".  >>  .  | \n",
      "She  >>  she  | \n",
      "is  >>  be  | \n",
      "good  >>  good  | \n",
      "at  >>  at  | \n",
      "many  >>  many  | \n",
      "skills  >>  skill  | \n",
      "like  >>  like  | \n",
      "dancing  >>  dancing  | \n",
      ",  >>  ,  | \n",
      "running  >>  running  | \n",
      ",  >>  ,  | \n",
      "singing  >>  singing  | \n",
      ",  >>  ,  | \n",
      "playing  >>  play  | \n",
      ".  >>  .  | \n",
      "She  >>  she  | \n",
      "also  >>  also  | \n",
      "likes  >>  like  | \n",
      "eating  >>  eat  | \n",
      "Pav  >>  Pav  | \n",
      "Bhagi  >>  Bhagi  | \n",
      ".  >>  .  | \n",
      "she  >>  she  | \n",
      "has  >>  have  | \n",
      "a  >>  a  | \n",
      "\n",
      "  >>  \n",
      "  | \n",
      "habit  >>  habit  | \n",
      "of  >>  of  | \n",
      "fishing  >>  fishing  | \n",
      "and  >>  and  | \n",
      "swimming  >>  swim  | \n",
      "too  >>  too  | \n",
      ".  >>  .  | \n",
      "Besides  >>  besides  | \n",
      "all  >>  all  | \n",
      "this  >>  this  | \n",
      ",  >>  ,  | \n",
      "she  >>  she  | \n",
      "is  >>  be  | \n",
      "a  >>  a  | \n",
      "wonderful  >>  wonderful  | \n",
      "at  >>  at  | \n",
      "cooking  >>  cook  | \n",
      "too  >>  too  | \n",
      ".  >>  .  | \n",
      "\n",
      "  >>  \n",
      "  | \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "sentences = []\n",
    "for token in doc:\n",
    "    sentences.append(token.text)\n",
    "    print(token.text, \" >> \", token.lemma_, \" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "140270f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Latha', 'is', 'very', 'multi', 'talented', 'girl', '.', 'She', 'is', 'good', 'at', 'many', 'skills', 'like', 'dancing', ',', 'running', ',', 'singing', ',', 'playing', '.', 'She', 'also', 'likes', 'eating', 'Pav', 'Bhagi', '.', 'she', 'has', 'a', '\\n', 'habit', 'of', 'fishing', 'and', 'swimming', 'too', '.', 'Besides', 'all', 'this', ',', 'she', 'is', 'a', 'wonderful', 'at', 'cooking', 'too', '.', '\\n'] \n",
      "\n",
      "Latha is very multi talented girl . She is good at many skills like dancing , running , singing , playing . She also likes eating Pav Bhagi . she has a \n",
      " habit of fishing and swimming too . Besides all this , she is a wonderful at cooking too . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sentences,\"\\n\")\n",
    "\n",
    "#joining all the base words \n",
    "final_sent = ' '.join(sentences)\n",
    "print(final_sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
